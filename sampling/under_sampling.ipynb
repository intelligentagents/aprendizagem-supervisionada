{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Comparison of the different under-sampling algorithms\n\n\nThe following example attends to make a qualitative comparison between the\ndifferent under-sampling algorithms available in the imbalanced-learn package.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>\n# License: MIT\n\nfrom collections import Counter\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.datasets import make_classification\nfrom sklearn.svm import LinearSVC\nfrom sklearn.linear_model import LogisticRegression\n\nfrom imblearn.pipeline import make_pipeline\nfrom imblearn.under_sampling import (ClusterCentroids, RandomUnderSampler,\n                                     NearMiss,\n                                     InstanceHardnessThreshold,\n                                     CondensedNearestNeighbour,\n                                     EditedNearestNeighbours,\n                                     RepeatedEditedNearestNeighbours,\n                                     AllKNN,\n                                     NeighbourhoodCleaningRule,\n                                     OneSidedSelection)\nprint(__doc__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following function will be used to create toy dataset. It using the\n``make_classification`` from scikit-learn but fixing some parameters.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def create_dataset(n_samples=1000, weights=(0.01, 0.01, 0.98), n_classes=3,\n                   class_sep=0.8, n_clusters=1):\n    return make_classification(n_samples=n_samples, n_features=2,\n                               n_informative=2, n_redundant=0, n_repeated=0,\n                               n_classes=n_classes,\n                               n_clusters_per_class=n_clusters,\n                               weights=list(weights),\n                               class_sep=class_sep, random_state=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following function will be used to plot the sample space after resampling\nto illustrate the characteristic of an algorithm.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plot_resampling(X, y, sampling, ax):\n    X_res, y_res = sampling.fit_resample(X, y)\n    ax.scatter(X_res[:, 0], X_res[:, 1], c=y_res, alpha=0.8, edgecolor='k')\n    # make nice plotting\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.get_xaxis().tick_bottom()\n    ax.get_yaxis().tick_left()\n    ax.spines['left'].set_position(('outward', 10))\n    ax.spines['bottom'].set_position(('outward', 10))\n    return Counter(y_res)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following function will be used to plot the decision function of a\nclassifier given some data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plot_decision_function(X, y, clf, ax):\n    plot_step = 0.02\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n                         np.arange(y_min, y_max, plot_step))\n\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    ax.contourf(xx, yy, Z, alpha=0.4)\n    ax.scatter(X[:, 0], X[:, 1], alpha=0.8, c=y, edgecolor='k')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prototype generation: under-sampling by generating new samples\n##############################################################################\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "``ClusterCentroids`` under-samples by replacing the original samples by the\ncentroids of the cluster found.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 6))\nX, y = create_dataset(n_samples=5000, weights=(0.01, 0.05, 0.94),\n                      class_sep=0.8)\n\nclf = LinearSVC().fit(X, y)\nplot_decision_function(X, y, clf, ax1)\nax1.set_title('Linear SVC with y={}'.format(Counter(y)))\nsampler = ClusterCentroids(random_state=0)\nclf = make_pipeline(sampler, LinearSVC())\nclf.fit(X, y)\nplot_decision_function(X, y, clf, ax2)\nax2.set_title('Decision function for {}'.format(sampler.__class__.__name__))\nplot_resampling(X, y, sampler, ax3)\nax3.set_title('Resampling using {}'.format(sampler.__class__.__name__))\nfig.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prototype selection: under-sampling by selecting existing samples\n##############################################################################\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The algorithm performing prototype selection can be subdivided into two\ngroups: (i) the controlled under-sampling methods and (ii) the cleaning\nunder-sampling methods.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With the controlled under-sampling methods, the number of samples to be\nselected can be specified. ``RandomUnderSampler`` is the most naive way of\nperforming such selection by randomly selecting a given number of samples by\nthe targetted class.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 6))\nX, y = create_dataset(n_samples=5000, weights=(0.01, 0.05, 0.94),\n                      class_sep=0.8)\n\nclf = LinearSVC().fit(X, y)\nplot_decision_function(X, y, clf, ax1)\nax1.set_title('Linear SVC with y={}'.format(Counter(y)))\nsampler = RandomUnderSampler(random_state=0)\nclf = make_pipeline(sampler, LinearSVC())\nclf.fit(X, y)\nplot_decision_function(X, y, clf, ax2)\nax2.set_title('Decision function for {}'.format(sampler.__class__.__name__))\nplot_resampling(X, y, sampler, ax3)\nax3.set_title('Resampling using {}'.format(sampler.__class__.__name__))\nfig.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "``NearMiss`` algorithms implement some heuristic rules in order to select\nsamples. NearMiss-1 selects samples from the majority class for which the\naverage distance of the $k$` nearest samples of the minority class is\nthe smallest. NearMiss-2 selects the samples from the majority class for\nwhich the average distance to the farthest samples of the negative class is\nthe smallest. NearMiss-3 is a 2-step algorithm: first, for each minority\nsample, their :$m$ nearest-neighbors will be kept; then, the majority\nsamples selected are the on for which the average distance to the $k$\nnearest neighbors is the largest.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, ((ax1, ax2), (ax3, ax4), (ax5, ax6)) = plt.subplots(3, 2,\n                                                         figsize=(15, 25))\nX, y = create_dataset(n_samples=5000, weights=(0.1, 0.2, 0.7), class_sep=0.8)\n\nax_arr = ((ax1, ax2), (ax3, ax4), (ax5, ax6))\nfor ax, sampler in zip(ax_arr, (NearMiss(version=1),\n                                NearMiss(version=2),\n                                NearMiss(version=3))):\n    clf = make_pipeline(sampler, LinearSVC())\n    clf.fit(X, y)\n    plot_decision_function(X, y, clf, ax[0])\n    ax[0].set_title('Decision function for {}-{}'.format(\n        sampler.__class__.__name__, sampler.version))\n    plot_resampling(X, y, sampler, ax[1])\n    ax[1].set_title('Resampling using {}-{}'.format(\n        sampler.__class__.__name__, sampler.version))\nfig.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "``EditedNearestNeighbours`` removes samples of the majority class for which\ntheir class differ from the one of their nearest-neighbors. This sieve can be\nrepeated which is the principle of the\n``RepeatedEditedNearestNeighbours``. ``AllKNN`` is slightly different from\nthe ``RepeatedEditedNearestNeighbours`` by changing the $k$ parameter\nof the internal nearest neighors algorithm, increasing it at each iteration.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, ((ax1, ax2), (ax3, ax4), (ax5, ax6)) = plt.subplots(3, 2,\n                                                         figsize=(15, 25))\nX, y = create_dataset(n_samples=500, weights=(0.2, 0.3, 0.5), class_sep=0.8)\n\nax_arr = ((ax1, ax2), (ax3, ax4), (ax5, ax6))\nfor ax, sampler in zip(ax_arr, (\n        EditedNearestNeighbours(),\n        RepeatedEditedNearestNeighbours(),\n        AllKNN(allow_minority=True))):\n    clf = make_pipeline(sampler, LinearSVC())\n    clf.fit(X, y)\n    plot_decision_function(X, y, clf, ax[0])\n    ax[0].set_title('Decision function for {}'.format(\n        sampler.__class__.__name__))\n    plot_resampling(X, y, sampler, ax[1])\n    ax[1].set_title('Resampling using {}'.format(\n        sampler.__class__.__name__))\nfig.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "``CondensedNearestNeighbour`` makes use of a 1-NN to iteratively decide if a\nsample should be kept in a dataset or not. The issue is that\n``CondensedNearestNeighbour`` is sensitive to noise by preserving the noisy\nsamples. ``OneSidedSelection`` also used the 1-NN and use ``TomekLinks`` to\nremove the samples considered noisy. The ``NeighbourhoodCleaningRule`` use a\n``EditedNearestNeighbours`` to remove some sample. Additionally, they use a 3\nnearest-neighbors to remove samples which do not agree with this rule.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, ((ax1, ax2), (ax3, ax4), (ax5, ax6)) = plt.subplots(3, 2,\n                                                         figsize=(15, 25))\nX, y = create_dataset(n_samples=500, weights=(0.2, 0.3, 0.5), class_sep=0.8)\n\nax_arr = ((ax1, ax2), (ax3, ax4), (ax5, ax6))\nfor ax, sampler in zip(ax_arr, (\n        CondensedNearestNeighbour(random_state=0),\n        OneSidedSelection(random_state=0),\n        NeighbourhoodCleaningRule())):\n    clf = make_pipeline(sampler, LinearSVC())\n    clf.fit(X, y)\n    plot_decision_function(X, y, clf, ax[0])\n    ax[0].set_title('Decision function for {}'.format(\n        sampler.__class__.__name__))\n    plot_resampling(X, y, sampler, ax[1])\n    ax[1].set_title('Resampling using {}'.format(\n        sampler.__class__.__name__))\nfig.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "``InstanceHardnessThreshold`` uses the prediction of classifier to exclude\nsamples. All samples which are classified with a low probability will be\nremoved.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 6))\nX, y = create_dataset(n_samples=5000, weights=(0.01, 0.05, 0.94),\n                      class_sep=0.8)\n\nclf = LinearSVC().fit(X, y)\nplot_decision_function(X, y, clf, ax1)\nax1.set_title('Linear SVC with y={}'.format(Counter(y)))\nsampler = InstanceHardnessThreshold(\n    random_state=0, estimator=LogisticRegression(solver='lbfgs',\n                                                 multi_class='auto'))\nclf = make_pipeline(sampler, LinearSVC())\nclf.fit(X, y)\nplot_decision_function(X, y, clf, ax2)\nax2.set_title('Decision function for {}'.format(sampler.__class__.__name__))\nplot_resampling(X, y, sampler, ax3)\nax3.set_title('Resampling using {}'.format(sampler.__class__.__name__))\nfig.tight_layout()\n\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}